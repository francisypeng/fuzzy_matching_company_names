{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of a large NYSE dataset and fuzzy matching on company names from two different datasets.\n",
    "Francis Peng <br>\n",
    "BA Economics/Mathematics, Music - May 2023 <br>\n",
    "Emory University Department of Economics\n",
    "\n",
    "Parts of the following notebooks are work in progress denoted by the markers **_Begin Work in Progress_** and **_End Work in Progress_**\n",
    "\n",
    "The following notebook desscribes and executes the process of cleaning a large dataset of NYSE stock listings as well as matching company names from two different datasets. In this process, the rapidfuzz library is used to implement fuzzy matching. Fuzzy matching is needed as the same company may appear differently in the two datasets. For example, the same company might be listed as \"X Corporation\" in one dataset and \"X Corp\" in the other. These are the same comapny, and thus should be matched as such.\n",
    "\n",
    "This fuzzy matching process was developed for the purposes of the following working papers:\n",
    "\n",
    "    Fohlin, Caroline (2020) “Crisis and Innovation:  The Transformation of the New York Stock Exchange from the Great War to the Great Depression,” working paper, Emory University.\n",
    "\n",
    "    Fohlin, Caroline and Phoebe Lei (2020) “The Determinants of the Volume of Initial Public Offerings During the Great Depression and World War II,” working paper, Emory University\n",
    "\n",
    "\n",
    "To protect the integrity of these unpublished materials, the source datasets are not output in their entirety anywhere in the following notebook. Only the final result table has been displayed to show that the process does indeed work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing rapidfuzz.\n",
    "# !pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Begin Work in Progreess_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the large dataset of NYSE listings.\n",
    "The big dataset contains companies listed on the NYSE (aka CRSP dataset)\n",
    "\n",
    "The cleaning of this dataset aims to achieve the following: <br>\n",
    "1. Standardize the naming conventions of companies.\n",
    "2. Fill in missing company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_stata('full_dataset.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2758\n",
      "1574\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.unique(df['comnam']))) # unique company names\n",
    "print(len(pd.unique(df['permco']))) # unique idnetifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head() # taking a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up company names\n",
    "df['comnam'] = df['comnam'].str.upper()\n",
    "df['comnam'] = df['comnam'].replace(\n",
    "    regex={',':'', '\\.':'', '-':' '}).replace(\n",
    "    regex={'CORPORATION':'CO', 'INCORPORATED':'INC', 'COMPANY':'CO', 'LIMITED':'LTD'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2508\n",
      "1574\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.unique(df['comnam']))) \n",
    "print(len(pd.unique(df['permco'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = df.dropna(subset=['permco'])\n",
    "# print(len(pd.unique(test['permco'])))\n",
    "# print(len(test))\n",
    "# print(len(pd.unique(test['comnam'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comnam'].replace('', np.nan, inplace = True) # replacing empty strings with null values\n",
    "noblank = df.dropna(subset=['comnam']) # dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.unique(noblank['permco'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we find that there are in fact permcos where no row exists where there is a comnam, i.e., some permcos are unidentifiable with comnam. The code directly below gives us the number of such permcos and the specific permcos that are unidentifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588\n"
     ]
    }
   ],
   "source": [
    "a = pd.unique(noblank['permco'])\n",
    "b = pd.unique(df['permco'])\n",
    "print(len(np.setdiff1d(b, a)))\n",
    "c = np.asarray(np.setdiff1d(b, a))\n",
    "np.savetxt(\"permco_missing_comnam.csv\", c, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[df['permco'] == 22195].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.unique(df.loc[df['permco'] == 56225, 'comnam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of this result which will be dealt with later, we continue with the cleaning. In particular, we now fill in comnam for permcos that are identifiable.\n",
    "\n",
    "1. First, we create a dataframe from the existing dataframe such that we have only comnam and permco.\n",
    "2. Then, we use this dataframe and join it with the original dataframe on permco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.\n",
    "# Create a dataframe with only comnam and permco\n",
    "working_df = df[['comnam', 'permco']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.dropna(subset = ['comnam'], inplace = True)\n",
    "working_df.dropna(subset = ['permco'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.drop_duplicates(subset = ['comnam'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df.sort_values('comnam', key = lambda x:x.str.len(), ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df.drop_duplicates(subset = ['permco'], keep = 'first' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(working_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df.rename(columns={'comnam':'new_name'})\n",
    "new_df = df.join(working_df.set_index('permco'), on='permco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('full_dataset_with_comnam.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_End Work in Progress_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the two datasets\n",
    "dataA = pd.read_excel('Data.xlsx')\n",
    "# dataB = pd.read_excel('CRSP.xlsx')\n",
    "dataB = new_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataB = dataB.rename(columns={'new_name':'company'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dataAW = dataA.dropna(subset = ['Company Name']).drop_duplicates('Company Name', keep = 'first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBW = dataB.dropna(subset = ['company']).drop_duplicates('company', keep = 'first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing common terms with abbreviations, deleting punctuation, making everything uppercase for ease of reading.\n",
    "dataAW['N_Company Name'] = dataAW['Company Name'].str.upper()\n",
    "dataBW['N_company'] = dataBW['company'].str.upper()\n",
    "\n",
    "dataAW['N_Company Name'] = dataAW['N_Company Name'].replace(\n",
    "    regex={',':'', '\\.':'', '-':' '}).replace(\n",
    "    regex={'CORPORATION':'CO', 'INCORPORATED':'INC', 'COMPANY':'CO', 'LIMITED':'LTD'}\n",
    ")\n",
    "\n",
    "dataBW['N_company'] = dataBW['N_company'].replace(\n",
    "    regex={',':'', '\\.':'', '-':' '}).replace(\n",
    "    regex={'CORPORATION':'CO', 'INCORPORATED':'INC', 'COMPANY':'CO', 'LIMITED':'LTD'}\n",
    ")\n",
    "\n",
    "#dataAW['N_Company Name'] = dataAW['N_Company Name'].replace(\n",
    "#    regex={',':'', '\\.':'', '-':' '}).replace(\n",
    "#    regex={'CORPORATION':'', 'INCORPORATED':'', 'COMPANY':''}).replace(\n",
    "#    regex={'LTD':'', 'CORP':'', 'INC':''}).replace(\n",
    "#    regex={'CO':''}\n",
    "#)\n",
    "\n",
    "#dataBW['N_company'] = dataBW['N_company'].replace(\n",
    "#    regex={',':'', '\\.':'', '-':' '}).replace(\n",
    "#    regex={'CORPORATION':'', 'INCORPORATED':'', 'COMPANY':''}).replace(\n",
    "#    regex={'LTD':'', 'CORP':'', 'INC':''}).replace(\n",
    "#    regex={'CO':''}\n",
    "#)\n",
    "\n",
    "dataAW = dataAW.dropna(subset = ['N_Company Name']).drop_duplicates('N_Company Name', keep = 'first').reset_index(drop=True)\n",
    "dataBW = dataBW.dropna(subset = ['N_company']).drop_duplicates('N_company', keep = 'first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching using the fuzzywuzzy library\n",
    "results_df = pd.DataFrame(columns = ['Match_Score', 'N_AWname', 'N_BWName', 'AWname'])\n",
    "for index, row in dataAW.iterrows():\n",
    "    result = process.extractOne(row['N_Company Name'], dataBW['N_company'])\n",
    "    #if result[1] >= 86:\n",
    "    results_df = results_df.append({'Match_Score':result[1], 'N_AWname':row['N_Company Name'], 'N_BWName':result[0], 'AWname':row['Company Name']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting values by match score given by fuzzywuzzy\n",
    "results_final = results_df.sort_values(by = ['Match_Score'], ascending = False).drop_duplicates('N_AWname', keep = 'first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match_Score</th>\n",
       "      <th>N_AWname</th>\n",
       "      <th>N_BWName</th>\n",
       "      <th>AWname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERAL ICE CREAM CORP</td>\n",
       "      <td>GENERAL ICE CREAM CORP</td>\n",
       "      <td>General Ice Cream Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>GREAT NORTHERN RAILWAY CO</td>\n",
       "      <td>GREAT NORTHERN RAILWAY CO</td>\n",
       "      <td>Great Northern Railway Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>GENERAL MILLS INC</td>\n",
       "      <td>GENERAL MILLS INC</td>\n",
       "      <td>General Mills, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>NATIONAL BISCUIT CO</td>\n",
       "      <td>NATIONAL BISCUIT CO</td>\n",
       "      <td>National Biscuit Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>SUN OIL CO</td>\n",
       "      <td>SUN OIL CO</td>\n",
       "      <td>Sun Oil Co.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Match_Score                   N_AWname                   N_BWName  \\\n",
       "0        100.0     GENERAL ICE CREAM CORP     GENERAL ICE CREAM CORP   \n",
       "1        100.0  GREAT NORTHERN RAILWAY CO  GREAT NORTHERN RAILWAY CO   \n",
       "2        100.0          GENERAL MILLS INC          GENERAL MILLS INC   \n",
       "3        100.0        NATIONAL BISCUIT CO        NATIONAL BISCUIT CO   \n",
       "4        100.0                 SUN OIL CO                 SUN OIL CO   \n",
       "\n",
       "                           AWname  \n",
       "0         General Ice Cream Corp.  \n",
       "1  Great Northern Railway Company  \n",
       "2             General Mills, Inc.  \n",
       "3        National Biscuit Company  \n",
       "4                     Sun Oil Co.  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a csv file\n",
    "output = results_final\n",
    "output.to_csv('10_27_rapidfuzz.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
